{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08afbec8-de8e-4b70-86f4-8baf2fbb920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models import resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa733b37-d372-4958-ba05-71eb1d5f7cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent dir for loading helpers\n",
    "import sys\n",
    "sys.path.insert(1, '../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e3afe4-f1a3-4b92-9afe-39ef8839f950",
   "metadata": {},
   "source": [
    "## Load RadImageNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef97ec0-f339-4cea-bd80-370ef2fa00a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import radimagenet\n",
    "\n",
    "backbone = radimagenet.RadImageNetBackbone()\n",
    "classifier = radimagenet.RadImageNetClassifier(num_class=1)\n",
    "\n",
    "backbone.load_state_dict(torch.load(\"../models/radimagenet_resnet50.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cecea9bd-d28b-43c1-8ab9-25a2cfbe17be",
   "metadata": {},
   "outputs": [],
   "source": [
    "radimagenet_model = nn.Sequential(backbone, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27d1c5d-f458-49bd-be0b-606fb27ff112",
   "metadata": {},
   "source": [
    "## Load ImageNet Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7612244-6643-4e89-8816-c3dee9dcca73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb_weights = torch.load(\"../models/rgb_3c_model_89.pth\", map_location='cpu', weights_only=False)\n",
    "rgb_model = torchvision.models.get_model(\"resnet50\", weights=None, num_classes=1000)\n",
    "rgb_model.load_state_dict(rgb_weights[\"model\"])\n",
    "# rgb_model = torch.nn.Sequential(*list(rgb_model.children())[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40203955-ee72-4097-b504-16777cf2700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_model = torchvision.models.get_model(\"resnet50\", weights=None, num_classes=1000)\n",
    "rgb_model.load_state_dict(rgb_weights[\"model\"])\n",
    "\n",
    "rgb_model = torch.nn.Sequential(*list(rgb_model.children())[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ca89a1-cd8d-40dc-8779-b4325f951440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_model = get_model(\"resnet50\", weights=None, num_classes=1000)\n",
    "# weights = torch.load(\"model.pth\", map_location='cpu', weights_only=False)\n",
    "# full_model.load_state_dict(weights[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef291941-622c-4865-9bfb-1adb2cc158a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_backbone = torch.nn.Sequential(*list(backbone.children())[:9])\n",
    "# model2_classifier = RadImageNetClassifier(num_class=num_class)\n",
    "model2_comparable = nn.Sequential(model2_backbone, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a918c2fd-7e0b-443f-9df0-8dfcd425f985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f202a9b-cb3c-450e-a8bd-69eefd11fd33",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13b4bd03-97a5-48ab-89b6-051f5ff12668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "from torch import float32 as tfloat32\n",
    "\n",
    "# crop dictionary of calculated dataset means and std devs\n",
    "CROP_DICT = {\n",
    "    # data      mean         std\n",
    "    'cxr14': [[162.7414], [44.0700]],\n",
    "    'openi': [[157.6150], [41.8371]],\n",
    "    'jsrt': [[161.7889], [41.3950]],\n",
    "    'padchest': [[160.3638], [44.8449]],\n",
    "}\n",
    "\n",
    "# arch segmented dictionary of calculated dataset means and std devs\n",
    "ARCH_SEG_DICT = {\n",
    "    # data       mean        std\n",
    "    'cxr14': [[128.2716], [76.7148]],\n",
    "    'openi': [[127.7211], [69.7704]],\n",
    "    'jsrt': [[139.9666], [72.4017]],\n",
    "    'padchest': [[129.5006], [72.6308]],\n",
    "    'padcxr14': [[128.8861], [74.6728]]\n",
    "}\n",
    "\n",
    "# lung segmented dictionary of calculated dataset means and std devs\n",
    "LUNG_SEG_DICT = {\n",
    "    # data       mean        std\n",
    "    'cxr14': [[60.6809], [68.9660]],\n",
    "    'openi': [[60.5483], [66.5276]],\n",
    "    'jsrt': [[66.5978], [72.6493]],\n",
    "    'padchest': [[60.5482], [66.5276]],\n",
    "    'padcxr14': [[60.61455], [67.7468]]\n",
    "}\n",
    "\n",
    "\n",
    "def get_cxr_eval_transforms(crop_size, normalise):\n",
    "    \"\"\"\n",
    "    Returns evaluation transforms for CXR images. Pass in target \n",
    "    crop size and the normalisation method for target dataset.\n",
    "    \"\"\"\n",
    "    cxr_transform_list = [\n",
    "        v2.ToImage(),\n",
    "        v2.Resize(size=crop_size, antialias=True),\n",
    "        v2.ToDtype(tfloat32, scale=False),\n",
    "        normalise\n",
    "    ]\n",
    "    return v2.Compose(cxr_transform_list)\n",
    "\n",
    "\n",
    "def get_cxr_single_eval_transforms(crop_size, normalise):\n",
    "    \"\"\"\n",
    "    Returns evaluation transforms for single channel output CXR \n",
    "    images. Pass in target crop size and the normalisation method \n",
    "    for target dataset.\n",
    "    \"\"\"\n",
    "    cxr_transform_list = [\n",
    "        v2.ToImage(),\n",
    "        v2.Grayscale(1),\n",
    "        v2.Resize(size=crop_size, antialias=True),\n",
    "        v2.ToDtype(tfloat32, scale=False),\n",
    "        normalise,\n",
    "    ]\n",
    "    return v2.Compose(cxr_transform_list)\n",
    "\n",
    "\n",
    "def get_cxr_dataset_normalisation(dataset, process):\n",
    "    \"\"\"\n",
    "    Returns normalisation transform for given dataset/config. Pass \n",
    "    in dataset name and the image processing method used.\n",
    "\n",
    "    Args:\n",
    "    - dataset (str): Name of CXR dataset. Expects (\"cxr14\", \"padchest\", \"openi\", \"jsrt\").\n",
    "    - process (str): Name of CXR processing applied. Expects (\"crop\", \"arch\", \"lung\").\n",
    "\n",
    "    Returns:\n",
    "    - torchvision.transform.V2 normalize method.\n",
    "\n",
    "    \"\"\"\n",
    "    if process.lower() not in (\"crop\", \"arch\", \"lung\"):\n",
    "        raise ValueError(f\"Unexpected CXR processing type: \\\n",
    "            {process}! Please choose from (crop, arch, lung).\")\n",
    "    else:\n",
    "        if dataset.lower() not in (\"cxr14\", \"padchest\", \"openi\", \"jsrt\"):\n",
    "            raise ValueError(f\"Unexpected CXR dataset type: \\\n",
    "                {dataset}! Please choose from (cxr14, padchest, \\\n",
    "                openi, jsrt).\")\n",
    "        else:\n",
    "            return v2.Normalize(CROP_DICT[dataset.lower()][0],\n",
    "                                CROP_DICT[dataset.lower()][1]) \\\n",
    "                if process.lower() == \"crop\" \\\n",
    "                else \\\n",
    "                v2.Normalize(ARCH_SEG_DICT[dataset.lower()][0],\n",
    "                             ARCH_SEG_DICT[dataset.lower()][1]) \\\n",
    "                if process.lower() == \"arch\" \\\n",
    "                else v2.Normalize(LUNG_SEG_DICT[dataset.lower()][0],\n",
    "                                  LUNG_SEG_DICT[dataset.lower()][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71131755-b7b0-47fb-afdc-7e6dbed3bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# from helpers.cxr import get_cxr_dataset_normalisation, get_cxr_eval_transforms, get_cxr_single_eval_transforms \n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ImageFolderWithPaths(ImageFolder):\n",
    "    \"\"\"Modifies torchviison ImageFolder to return (img, label, img_path)\"\"\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img, label = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "\n",
    "        path = self.imgs[index][0]\n",
    "\n",
    "        return (img, label, path)\n",
    "\n",
    "def load_dataset_with_paths_direct_path(dataset_path, dataset_name, process=\"arch\", \n",
    "    crop_size=512, batch_size=4, shuffle=True, single_channel=False):\n",
    "    \"\"\"Wrapper helper to load a dataset with img paths.\"\"\"\n",
    "    \n",
    "    dataset = ImageFolderWithPaths(\n",
    "        root = dataset_path,\n",
    "        transform = get_cxr_eval_transforms(\n",
    "            crop_size = crop_size,\n",
    "            normalise = get_cxr_dataset_normalisation(\n",
    "                dataset = dataset_name, \n",
    "                process = process\n",
    "                )\n",
    "            ) if not single_channel else\n",
    "        get_cxr_single_eval_transforms(\n",
    "            crop_size = crop_size,\n",
    "            normalise = get_cxr_dataset_normalisation(\n",
    "                dataset = dataset_name, \n",
    "                process = process\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a3cd38-9f07-4710-9bbf-932a9e9e057b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
